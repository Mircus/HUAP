{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUAPize Your LangChain App\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mircus/HUAP/blob/main/notebooks/HUAPize_LangChain.ipynb)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Mircus/HUAP/main/HUAP-logo.png\" alt=\"HUAP Logo\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "**Add full traceability to your LangChain app — zero code changes to your chain logic.**\n",
    "\n",
    "HUAP's `HuapCallbackHandler` plugs into LangChain's callback system.\n",
    "Every LLM call, tool invocation, and chain step is automatically captured as a JSONL trace.\n",
    "\n",
    "No API keys needed — this demo uses LangChain's built-in `FakeListChatModel`.\n",
    "\n",
    "| Before HUAP | After HUAP |\n",
    "|---|---|\n",
    "| Chain runs, prints output | Every action recorded as `trace.jsonl` |\n",
    "| \"It worked yesterday\" | Deterministic replay + diff |\n",
    "| No regression testing | CI gates with golden baselines |\n",
    "| No audit trail | Shareable HTML reports |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install HUAP from PyPI + LangChain (core only — no API keys needed)\n!pip install -q huap-core langchain-core",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: A Real LangChain Chain (before HUAP)\n",
    "\n",
    "A standard LangChain pipeline: **prompt template → chat model → output parser**.\n",
    "\n",
    "We use `FakeListChatModel` so it runs without API keys, but the chain is 100% real LangChain —\n",
    "swap in `ChatOpenAI` or `ChatAnthropic` and everything works the same."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === BEFORE HUAP: a real LangChain chain, no tracing ===\n",
    "\n",
    "from langchain_core.language_models import FakeListChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Fake LLM — returns canned responses (swap with ChatOpenAI for production)\n",
    "llm = FakeListChatModel(responses=[\n",
    "    \"HUAP is a trace-first framework for building deterministic, testable AI agents. \"\n",
    "    \"It records every LLM call, tool use, and decision as a JSONL trace, enabling \"\n",
    "    \"replay, diff, and CI gating.\",\n",
    "])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful technical writer.\"),\n",
    "    (\"human\", \"Explain {topic} in 2-3 sentences.\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\"topic\": \"HUAP framework\"})\n",
    "print(result)\n",
    "print(\"\\n--- No trace. No diff. No CI. Just stdout. ---\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: HUAPize It — One Line Change\n",
    "\n",
    "Same chain. Same logic. **Zero changes to your pipeline code.**\n",
    "\n",
    "Just:\n",
    "1. Create a `HuapCallbackHandler`\n",
    "2. Pass it in `config={\"callbacks\": [handler]}`\n",
    "3. Call `handler.flush()`\n",
    "\n",
    "LangChain fires the callbacks automatically — every LLM request, response, chain start/end,\n",
    "tool call, and error is captured in the HUAP trace."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === AFTER HUAP: same chain, now fully traced — automatically ===\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from hu_core.adapters.langchain import HuapCallbackHandler\n",
    "\n",
    "Path(\"traces\").mkdir(exist_ok=True)\n",
    "\n",
    "# Same chain as before — no changes\n",
    "llm = FakeListChatModel(responses=[\n",
    "    \"HUAP is a trace-first framework for building deterministic, testable AI agents. \"\n",
    "    \"It records every LLM call, tool use, and decision as a JSONL trace, enabling \"\n",
    "    \"replay, diff, and CI gating.\",\n",
    "])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# >>> THE ONLY ADDITION: create a HUAP handler <<<\n",
    "handler = HuapCallbackHandler(out=\"traces/langchain_demo.jsonl\", run_name=\"langchain_demo\")\n",
    "\n",
    "# Run with callbacks — LangChain fires all events automatically\n",
    "result = chain.invoke(\n",
    "    {\"topic\": \"HUAP framework\"},\n",
    "    config={\"callbacks\": [handler]},\n",
    ")\n",
    "handler.flush()\n",
    "\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "# Show the auto-captured trace events\n",
    "print(\"=\" * 60)\n",
    "print(\"Auto-captured trace events:\")\n",
    "print(\"=\" * 60)\n",
    "with open(\"traces/langchain_demo.jsonl\") as f:\n",
    "    for line in f:\n",
    "        event = json.loads(line)\n",
    "        kind = event.get(\"kind\", \"?\")\n",
    "        name = event.get(\"name\", \"?\")\n",
    "        data = event.get(\"data\", {})\n",
    "        label = data.get(\"tool\", data.get(\"model\", data.get(\"node\", \"\")))\n",
    "        print(f\"  {kind:12s}  {name:20s}  {label}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Multi-Step Chain with Tools\n",
    "\n",
    "Real apps have multiple steps and tools. Let's build a slightly more complex chain\n",
    "and show that HUAP captures everything — including tool calls."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from langchain_core.language_models import FakeListChatModel\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.tools import tool\nfrom langchain_core.runnables import RunnableLambda\nfrom hu_core.adapters.langchain import HuapCallbackHandler\n\n@tool\ndef word_count(text: str) -> str:\n    \"\"\"Count words in the given text.\"\"\"\n    count = len(text.split())\n    return f\"Word count: {count}\"\n\n# Multi-step: generate → count words → summarize\nllm_step1 = FakeListChatModel(responses=[\n    \"AI agents are autonomous systems that perceive, reason, and act. \"\n    \"They use LLMs as their reasoning engine and tools for real-world interaction. \"\n    \"Traceability is critical for production deployment.\",\n])\n\nllm_step2 = FakeListChatModel(responses=[\n    \"Summary: 3-sentence overview of AI agents covering autonomy, LLMs, and traceability. \"\n    \"Word count is within target range.\",\n])\n\ngenerate_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a technical writer.\"),\n    (\"human\", \"Write a brief overview of {topic}.\"),\n])\n\nsummarize_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an editor.\"),\n    (\"human\", \"Summarize this draft and its word count:\\n\\nDraft: {draft}\\nWord count: {wc}\"),\n])\n\n# Build the chain\nmulti_chain = (\n    generate_prompt\n    | llm_step1\n    | StrOutputParser()\n    | RunnableLambda(lambda draft: {\n        \"draft\": draft,\n        \"wc\": word_count.invoke(draft),\n    })\n    | summarize_prompt\n    | llm_step2\n    | StrOutputParser()\n)\n\n# Run with HUAP tracing\nhandler2 = HuapCallbackHandler(out=\"traces/langchain_multi.jsonl\", run_name=\"langchain_multi\")\n\nresult = multi_chain.invoke(\n    {\"topic\": \"AI agents\"},\n    config={\"callbacks\": [handler2]},\n)\nhandler2.flush()\n\nprint(result)\nprint()\nprint(\"=\" * 60)\nprint(\"Multi-step trace events:\")\nprint(\"=\" * 60)\nimport json\nwith open(\"traces/langchain_multi.jsonl\") as f:\n    for line in f:\n        event = json.loads(line)\n        kind = event.get(\"kind\", \"?\")\n        name = event.get(\"name\", \"?\")\n        data = event.get(\"data\", {})\n        label = data.get(\"tool\", data.get(\"model\", data.get(\"node\", \"\")))\n        print(f\"  {kind:12s}  {name:20s}  {label}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Generate HTML Report\n",
    "\n",
    "Turn the raw JSONL into a standalone HTML report anyone can open in a browser."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!huap trace report traces/langchain_multi.jsonl --out reports/langchain_multi.html 2>&1 || true\n",
    "\n",
    "report = Path(\"reports/langchain_multi.html\")\n",
    "if report.exists():\n",
    "    from IPython.display import HTML, display\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HTML Report (inline):\")\n",
    "    print(\"=\" * 60)\n",
    "    display(HTML(report.read_text()))\n",
    "else:\n",
    "    print(\"The trace file is at: traces/langchain_multi.jsonl\")\n",
    "    print(\"View it with: huap trace view traces/langchain_multi.jsonl\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Prove Reproducibility — Diff Two Runs\n",
    "\n",
    "Run the same chain again and diff the two traces.\n",
    "With a fake LLM the outputs are identical — proving deterministic replay."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from langchain_core.language_models import FakeListChatModel\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\nfrom hu_core.adapters.langchain import HuapCallbackHandler\n\n# Second run — same chain, same inputs\nllm_step1_v2 = FakeListChatModel(responses=[\n    \"AI agents are autonomous systems that perceive, reason, and act. \"\n    \"They use LLMs as their reasoning engine and tools for real-world interaction. \"\n    \"Traceability is critical for production deployment.\",\n])\nllm_step2_v2 = FakeListChatModel(responses=[\n    \"Summary: 3-sentence overview of AI agents covering autonomy, LLMs, and traceability. \"\n    \"Word count is within target range.\",\n])\n\nmulti_chain_v2 = (\n    generate_prompt\n    | llm_step1_v2\n    | StrOutputParser()\n    | RunnableLambda(lambda draft: {\n        \"draft\": draft,\n        \"wc\": word_count.invoke(draft),\n    })\n    | summarize_prompt\n    | llm_step2_v2\n    | StrOutputParser()\n)\n\nhandler3 = HuapCallbackHandler(out=\"traces/langchain_multi_v2.jsonl\", run_name=\"langchain_multi_v2\")\nresult_v2 = multi_chain_v2.invoke(\n    {\"topic\": \"AI agents\"},\n    config={\"callbacks\": [handler3]},\n)\nhandler3.flush()\n\n# Diff the two runs\n!huap trace diff traces/langchain_multi.jsonl traces/langchain_multi_v2.jsonl",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What You Just Did\n",
    "\n",
    "| Before | After |\n",
    "|---|---|\n",
    "| A LangChain chain that runs and prints | A traced, diffable, CI-gatable agent system |\n",
    "| No audit trail | Every LLM call, tool use, and chain step in `trace.jsonl` |\n",
    "| \"It worked yesterday\" | Deterministic replay proves reproducibility |\n",
    "| No regression testing | `huap ci run` gates regressions against baselines |\n",
    "| No shareable artifacts | `trace.html` — send to anyone |\n",
    "\n",
    "### What changed in your code?\n",
    "\n",
    "**Three lines — zero changes to your chain logic:**\n",
    "\n",
    "```python\n",
    "from hu_core.adapters.langchain import HuapCallbackHandler\n",
    "\n",
    "handler = HuapCallbackHandler(out=\"traces/my_trace.jsonl\")\n",
    "chain.invoke(input, config={\"callbacks\": [handler]})\n",
    "handler.flush()\n",
    "```\n",
    "\n",
    "Your prompts, models, tools, and chains stay exactly the same.\n",
    "HUAP just observes — it never modifies your pipeline.\n",
    "\n",
    "### Using with a real LLM\n",
    "\n",
    "Replace `FakeListChatModel` with any LangChain chat model:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "```\n",
    "\n",
    "The `HuapCallbackHandler` captures everything automatically — model names, token usage, latency, and full message content.\n",
    "\n",
    "### Next steps\n",
    "- [GitHub](https://github.com/Mircus/HUAP) — source, examples, contributing guide\n",
    "- [Try HUAP](https://colab.research.google.com/github/Mircus/HUAP/blob/main/notebooks/Try_HUAP.ipynb) — the flagship demo notebook\n",
    "- [HUAPize Any Agent](https://colab.research.google.com/github/Mircus/HUAP/blob/main/notebooks/HUAPize_Agents.ipynb) — manual tracing for any framework\n",
    "- [Getting Started](https://github.com/Mircus/HUAP/blob/main/GETTING_STARTED.md) — full tutorial"
   ]
  }
 ]
}